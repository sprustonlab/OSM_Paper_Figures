{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad1fe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Set up matplotlib\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "# Check available GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Number of GPUs found: {num_gpus}\")\n",
    "\n",
    "def to_onehot(x, n):\n",
    "    x_onehot = np.zeros((len(x), n), dtype=np.int64)\n",
    "    for i in range(len(x)):\n",
    "        x_onehot[i, x[i]] = 1\n",
    "    return x_onehot\n",
    "\n",
    "def corr_finder(hidden_all, test_trials, tr_len, hidden_size):    \n",
    "    hidden_all = hidden_all.cpu().detach().numpy()\n",
    "    test0 = np.where(test_trials==0)[0]\n",
    "    test0_act = np.zeros((tr_len, hidden_size, len(test0)))\n",
    "    count = 0\n",
    "    for i in test0:\n",
    "        test0_act[:,:,count] = hidden_all[i*tr_len: (i+1)*tr_len,:]\n",
    "        count = count + 1\n",
    "\n",
    "    test1 = np.where(test_trials==1)[0]\n",
    "    test1_act = np.zeros((tr_len, hidden_size, len(test1)))\n",
    "    count = 0\n",
    "    for i in test1:\n",
    "        test1_act[:,:,count] = hidden_all[i*tr_len: (i+1)*tr_len,:]\n",
    "        count = count + 1\n",
    "    corrplot = np.corrcoef(np.mean(test0_act, axis=2), np.mean(test1_act, axis=2))\n",
    "    return corrplot\n",
    "\n",
    "class RNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNNCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.W_ih = nn.Parameter(torch.Tensor(hidden_size, input_size), requires_grad=True)\n",
    "        self.W_hh = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        feedforward_input = torch.mm(x.unsqueeze(0), self.W_ih.t())\n",
    "        recurrent_input = torch.mm(hidden.unsqueeze(0), self.W_hh.t())\n",
    "        \n",
    "        temperature = 1\n",
    "        \n",
    "        logits = (feedforward_input + recurrent_input) / temperature\n",
    "        \n",
    "        ## polynomial\n",
    "        new_hidden = torch.pow(logits, 8)\n",
    "        \n",
    "        new_hidden = new_hidden / torch.sum(new_hidden)\n",
    "    \n",
    "        return new_hidden.squeeze(0)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.rnn_cell = RNNCell(input_size, hidden_size)\n",
    "        \n",
    "        suggested_hh = torch.tensor(np.random.normal(0, 0.001, (hidden_size, hidden_size)), dtype=torch.float32)\n",
    "        suggested_ih = torch.tensor(np.random.normal(0, 0.001, (hidden_size, input_size)), dtype=torch.float32)\n",
    "        self.rnn_cell.W_ih.data = suggested_ih\n",
    "        self.rnn_cell.W_hh.data = suggested_hh\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.fc.weight.data = torch.tensor(np.random.normal(0, 1, (output_size, hidden_size)), dtype=torch.float32)\n",
    "        for param in self.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden_state = torch.zeros(x.size(0), self.hidden_size, dtype=x.dtype, device=x.device)\n",
    "\n",
    "        hidden_state[0,:] = self.rnn_cell(x[0, :], torch.zeros(self.rnn_cell.hidden_size, dtype=x.dtype, device=x.device))\n",
    "\n",
    "        for t in range(1, x.size(0)):\n",
    "            hidden_state[t,:] = self.rnn_cell(x[t, :], hidden_state[t-1,:].clone())\n",
    "  \n",
    "        out = self.fc(hidden_state)\n",
    "        \n",
    "        return out, hidden_state\n",
    "\n",
    "def run_simulation(gpu_id, seeds, shared_dict):\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "    device = torch.device(f\"cuda:{gpu_id}\")\n",
    "    \n",
    "    trial1x = np.array([1,1,1,1,1,1,2,2,2,2,1,1,1,4,6,1,1,1,5,5,1,1,0])\n",
    "    trial2x = np.array([1,1,1,1,1,1,3,3,3,3,1,1,1,4,4,1,1,1,5,6,1,1,0])\n",
    "\n",
    "    num_trials = 100\n",
    "    num_trials_train = 50\n",
    "    tr_len = len(trial1x)\n",
    "    OBS = len(np.unique(np.concatenate((trial1x, trial2x))))\n",
    "    input_size = OBS\n",
    "    hidden_size = 5000\n",
    "    output_size = OBS\n",
    "    epochs = 500\n",
    "    save_each = 10\n",
    "\n",
    "    loss_all = np.zeros((len(seeds), int(epochs/save_each)))\n",
    "    corr_curve = np.zeros((len(seeds), int(epochs/save_each), tr_len*2, tr_len*2))\n",
    "    accuracy_curve_all_test = np.zeros((len(seeds), int(epochs/save_each)))\n",
    "\n",
    "    for idx, seed in enumerate(seeds):\n",
    "        print(f'GPU {gpu_id}: Starting simulation {idx+1}/{len(seeds)} with seed {seed}')\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        rnn = RNN(input_size, hidden_size, output_size).to(device)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(rnn.parameters(), lr=0.2)\n",
    "        \n",
    "        total_train_time = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            found_trials = False\n",
    "            while not found_trials:\n",
    "                trials = np.random.choice(2, num_trials)\n",
    "                if np.sum(trials[:-1]==1)>1 and np.sum(trials[:-1]==0)>1:\n",
    "                    found_trials = True\n",
    "            x = np.zeros(num_trials*tr_len, dtype=np.int64)\n",
    "\n",
    "            for trial in range(len(trials)):\n",
    "                if trials[trial] == 0:\n",
    "                    x[trial*tr_len: (trial+1)*tr_len] = trial1x\n",
    "                else:\n",
    "                    x[trial*tr_len: (trial+1)*tr_len] = trial2x\n",
    "\n",
    "            x = to_onehot(x, OBS)\n",
    "            \n",
    "            train_x = torch.tensor(x[0:num_trials_train*tr_len], dtype=torch.float32).to(device)\n",
    "            test_x = torch.tensor(x[num_trials_train*tr_len:], dtype=torch.float32).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            input_x = train_x[:-1]\n",
    "\n",
    "            prediction, hidden_all = rnn(input_x)\n",
    "\n",
    "            loss = loss_func(prediction, train_x[1:].argmax(axis=1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "            total_train_time += epoch_duration\n",
    "            \n",
    "            if epoch % save_each == 0:\n",
    "                with torch.no_grad():\n",
    "                    pred_test, hidden_test = rnn(test_x)\n",
    "                    actual_reward = np.where(test_x[1:].cpu().argmax(axis=1) == 6)\n",
    "                    predicted_reward = np.where(pred_test.cpu().argmax(axis=1) == 6)\n",
    "                    accuracy = len(np.intersect1d(actual_reward, predicted_reward)) / len(actual_reward[0])\n",
    "                    accuracy_curve_all_test[idx, int(epoch/save_each)] = accuracy\n",
    "\n",
    "                    corr_curve[idx, int(epoch/save_each)] = corr_finder(hidden_test, trials[num_trials_train:], tr_len, hidden_size)\n",
    "\n",
    "                    loss_all[idx, int(epoch/save_each)] = loss.item()\n",
    "                \n",
    "                print(f\"GPU {gpu_id}, Sim {idx+1}, Epoch {epoch}/{epochs} - Loss: {loss.item():.4f} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        print(f\"GPU {gpu_id}: Simulation {idx+1} completed. Total training time: {total_train_time:.2f} seconds\")\n",
    "\n",
    "    shared_dict[gpu_id] = {\n",
    "        'loss_all': loss_all,\n",
    "        'corr_curve': corr_curve,\n",
    "        'accuracy_curve_all_test': accuracy_curve_all_test\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_simulations = 48\n",
    "    seeds_per_gpu = total_simulations // num_gpus\n",
    "    remaining_seeds = total_simulations % num_gpus\n",
    "\n",
    "    all_seeds = np.arange(200, 200 + total_simulations)\n",
    "    \n",
    "    manager = mp.Manager()\n",
    "    shared_dict = manager.dict()\n",
    "\n",
    "    processes = []\n",
    "    start_idx = 0\n",
    "    for i in range(num_gpus):\n",
    "        end_idx = start_idx + seeds_per_gpu + (1 if i < remaining_seeds else 0)\n",
    "        gpu_seeds = all_seeds[start_idx:end_idx]\n",
    "        p = mp.Process(target=run_simulation, args=(i, gpu_seeds, shared_dict))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        start_idx = end_idx\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    # Combine results from all GPUs\n",
    "    combined_loss_all = np.concatenate([shared_dict[i]['loss_all'] for i in range(num_gpus)])\n",
    "    combined_corr_curve = np.concatenate([shared_dict[i]['corr_curve'] for i in range(num_gpus)])\n",
    "    combined_accuracy_curve = np.concatenate([shared_dict[i]['accuracy_curve_all_test'] for i in range(num_gpus)])\n",
    "\n",
    "    print(\"\\nAll simulations completed. Saving results...\")\n",
    "\n",
    "    name_add = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    np.save(f'./polynomial_corr_curve_{name_add}', combined_corr_curve)\n",
    "    np.save(f'./polynomial_accuracy_curve_all_test_{name_add}', combined_accuracy_curve)\n",
    "    np.save(f'./polynomial_loss_all_{name_add}', combined_loss_all)\n",
    "\n",
    "    print(f\"Results saved with timestamp: {name_add}\")\n",
    "\n",
    "    print(\"\\nFinal Statistics:\")\n",
    "    print(f\"Average loss: {np.mean(combined_loss_all[:, -1]):.4f}\")\n",
    "    print(f\"Average accuracy: {np.mean(combined_accuracy_curve[:, -1]):.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.mean(combined_loss_all, axis=0))\n",
    "    plt.title(\"Average Loss Curve (Polynomial Activation)\")\n",
    "    plt.xlabel(\"Epochs (x10)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(np.mean(combined_accuracy_curve, axis=0))\n",
    "    plt.title(\"Average Accuracy Curve (Polynomial Activation)\")\n",
    "    plt.xlabel(\"Epochs (x10)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'polynomial_training_curves_{name_add}.png')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Training visualization completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b6570",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import matplotlib\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "\n",
    "print(\"Analysis of GPU RNN Model Results\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Constants\n",
    "tr_len = 23\n",
    "loss_thresh = 0.04\n",
    "decorr_thresh = 0.3  # New threshold for decorrelation analysis\n",
    "\n",
    "# Load data\n",
    "file_chosen = '2024-08-21-15-50'\n",
    "print(f\"Loading data from files with timestamp: {file_chosen}\")\n",
    "\n",
    "corr_curve = np.load(f'polynomial_corr_curve_{file_chosen}.npy')\n",
    "accuracy_curve_all_test = np.load(f'polynomial_accuracy_curve_all_test_{file_chosen}.npy')\n",
    "loss_all = np.load(f'polynomial_loss_all_{file_chosen}.npy')\n",
    "\n",
    "print(f\"Data loaded successfully.\")\n",
    "print(f\"Total number of simulations: {corr_curve.shape[0]}\")\n",
    "print(f\"Number of epochs: {corr_curve.shape[1]}\")\n",
    "print(f\"Correlation matrix size: {corr_curve.shape[2]}x{corr_curve.shape[3]}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Basic Statistics\n",
    "print(\"Basic Statistics:\")\n",
    "print(f\"Final mean loss: {np.mean(loss_all[:,-1]):.4f}\")\n",
    "print(f\"Final mean accuracy: {np.mean(accuracy_curve_all_test[:,-1]):.4f}\")\n",
    "\n",
    "# Number of runs with good convergence\n",
    "good_runs = (loss_all[:,-1] < loss_thresh).sum()\n",
    "print(f\"Number of runs with good convergence (loss < {loss_thresh}): {good_runs}\")\n",
    "print(f\"Percentage of good runs: {good_runs/loss_all.shape[0]*100:.2f}%\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Last time step mean correlation\n",
    "corr_avg_last_session = np.mean(corr_curve[:,-1,0:tr_len,tr_len:2*tr_len][loss_all[:,-1]<loss_thresh], axis=(1,2))\n",
    "print(\"Last time step mean correlation statistics:\")\n",
    "print(f\"Mean: {np.mean(corr_avg_last_session):.4f}\")\n",
    "print(f\"Median: {np.median(corr_avg_last_session):.4f}\")\n",
    "print(f\"Min: {np.min(corr_avg_last_session):.4f}\")\n",
    "print(f\"Max: {np.max(corr_avg_last_session):.4f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(\"Generating plots...\")\n",
    "\n",
    "# Plot mean correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sb.heatmap(np.mean(corr_curve[:,-1,0:tr_len,tr_len:2*tr_len][loss_all[:,-1]<loss_thresh], axis=0), \n",
    "           vmin=-1, vmax=1, cmap='icefire')\n",
    "for plot_line in [6,10,13,15,18,20]:\n",
    "    plt.axvline(plot_line, linestyle='--', color='gray')\n",
    "    plt.axhline(plot_line, linestyle='--', color='gray')\n",
    "    plt.axvline(plot_line+23, linestyle='--', color='gray')\n",
    "    plt.axhline(plot_line+23, linestyle='--', color='gray')\n",
    "plt.title('Softmax average correlation')\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean correlation matrix plotted.\")\n",
    "print(\"This heatmap shows the average correlation between different time steps across all good runs.\")\n",
    "print(\"The diagonal structure suggests temporal dependencies in the model's representations.\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Define regions for correlation analysis\n",
    "regions = [[0, 6], [10, 13], [15, 18], [20, 23]]\n",
    "other_regions = np.array([10, 13, 14, 15, 18, 19, 20, 23])\n",
    "\n",
    "# Create correlation matrices\n",
    "correlation_matrix_1 = np.zeros((tr_len, tr_len))\n",
    "for i, region_i in enumerate(regions):\n",
    "    for j, region_j in enumerate(regions):\n",
    "        if i != j:\n",
    "            correlation_matrix_1[region_i[0]:region_i[1], region_j[0]:region_j[1]] = 1\n",
    "\n",
    "correlation_matrix_2 = np.zeros((tr_len, tr_len))\n",
    "correlation_matrix_2[other_regions[3]:other_regions[5], other_regions[3]:other_regions[5]] = np.eye(other_regions[5] - other_regions[3])\n",
    "\n",
    "correlation_matrix_3 = np.zeros((tr_len, tr_len))\n",
    "correlation_matrix_3[other_regions[0]:other_regions[2], other_regions[0]:other_regions[2]] = np.eye(other_regions[2] - other_regions[0])\n",
    "\n",
    "correlation_matrices = [correlation_matrix_1, correlation_matrix_2, correlation_matrix_3]\n",
    "correlation_names = ['Off-diagonal', 'Pre-R2', 'Pre-R1']\n",
    "\n",
    "# Analyze decorrelation\n",
    "good_run_indices = np.where(loss_all[:,-1] < loss_thresh)[0]\n",
    "all_masks_matrix = np.full((3, len(good_run_indices), corr_curve.shape[1]), np.nan)\n",
    "\n",
    "corr_curve_lim = corr_curve[loss_all[:,-1] < loss_thresh]\n",
    "\n",
    "# Calculate mean values for all regions\n",
    "for i, session_n in enumerate(range(len(good_run_indices))):\n",
    "    corr_position_day = corr_curve_lim[session_n][:,0:tr_len, tr_len:2*tr_len]\n",
    "    for j, mask in enumerate(correlation_matrices):\n",
    "        mask_array = np.zeros_like(corr_position_day, dtype=bool)\n",
    "        mask_array += mask.astype(bool)\n",
    "        masked_a_array = np.ma.masked_array(corr_position_day, mask=~mask_array)\n",
    "        mean_values_array = masked_a_array.mean(axis=(1, 2))\n",
    "        all_masks_matrix[j, i, 0:len(mean_values_array)] = mean_values_array\n",
    "\n",
    "# Additional filtering based on decorrelation threshold\n",
    "final_means = all_masks_matrix[:, :, -1]\n",
    "good_decorr_runs = np.all(final_means < decorr_thresh, axis=0)\n",
    "filtered_indices = good_run_indices[good_decorr_runs]\n",
    "\n",
    "print(f\"Number of runs passing both loss and decorrelation thresholds: {np.sum(good_decorr_runs)}\")\n",
    "print(f\"Percentage of good runs passing decorrelation threshold: {np.sum(good_decorr_runs)/len(good_run_indices)*100:.2f}%\")\n",
    "\n",
    "# Plot individual correlation matrices as subplots\n",
    "n_plots = min(9, np.sum(good_decorr_runs))\n",
    "n_rows = math.ceil(n_plots / 3)\n",
    "n_cols = min(n_plots, 3)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 5*n_rows))\n",
    "axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "\n",
    "for i in range(n_plots):\n",
    "    sb.heatmap(corr_curve[:,-1,0:tr_len,tr_len:2*tr_len][filtered_indices[i],:,:], \n",
    "               vmin=-1, vmax=1, cmap='icefire', ax=axes[i])\n",
    "    for plot_line in [6,10,13,15,18,20]:\n",
    "        axes[i].axvline(plot_line, linestyle='--', color='gray')\n",
    "        axes[i].axhline(plot_line, linestyle='--', color='gray')\n",
    "        axes[i].axvline(plot_line+23, linestyle='--', color='gray')\n",
    "        axes[i].axhline(plot_line+23, linestyle='--', color='gray')\n",
    "    axes[i].set_title(f'Run {filtered_indices[i]+1}\\nLoss: {loss_all[filtered_indices[i],-1]:.4f}, '\n",
    "                      f'Acc: {accuracy_curve_all_test[filtered_indices[i],-1]:.4f}')\n",
    "\n",
    "for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Individual correlation matrices plotted for filtered runs.\")\n",
    "print(\"These plots show the variation in correlation patterns across different runs that pass both thresholds.\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Plot average accuracy curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.mean(accuracy_curve_all_test[filtered_indices], axis=0))\n",
    "plt.title('Average Accuracy Curve (Filtered Runs)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "print(\"Average accuracy curve plotted for filtered runs.\")\n",
    "print(f\"Final average accuracy for filtered runs: {np.mean(accuracy_curve_all_test[filtered_indices, -1]):.4f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Plot correlation distribution\n",
    "corr_all = np.mean(corr_curve[:,-1,0:tr_len,tr_len:2*tr_len][good_run_indices], axis=(1,2))\n",
    "num_points = len(corr_all)\n",
    "xjitter = np.random.normal(0, 0.1, num_points)\n",
    "yall = 6 * np.ones(num_points) + xjitter\n",
    "\n",
    "fig, axs = plt.subplots(1, figsize=(20, 10))\n",
    "axs.bar([1], np.mean(corr_all))\n",
    "axs.plot(corr_all, yall, 'o', color='black')\n",
    "axs.set_ylim(2, 10)\n",
    "axs.set_xlim(0, 1)\n",
    "axs.set_yticks([2, 4, 6, 8, 10], ['', '', 'softmax', '', ''])\n",
    "axs.set_title('Correlation Distribution (Filtered Runs)')\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation distribution plotted for filtered runs.\")\n",
    "print(\"This plot shows the distribution of mean correlations across filtered runs.\")\n",
    "print(f\"Mean correlation: {np.mean(corr_all):.4f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Plot correlation matrices\n",
    "for i, matrix in enumerate(correlation_matrices):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sb.heatmap(matrix)\n",
    "    plt.title(f'Correlation Matrix {i+1}: {correlation_names[i]}')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Correlation matrices plotted.\")\n",
    "print(\"These matrices show the regions we're interested in analyzing:\")\n",
    "print(\"Matrix 1: Off-diagonal regions\")\n",
    "print(\"Matrix 2: Pre-R2 region\")\n",
    "print(\"Matrix 3: Pre-R1 region\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Plot decorrelation analysis for filtered runs\n",
    "n_iter = 10\n",
    "subplots_per_fig = 20\n",
    "num_figures = math.ceil(np.sum(good_decorr_runs) / subplots_per_fig)\n",
    "\n",
    "for fig_num in range(num_figures):\n",
    "    start_idx = fig_num * subplots_per_fig\n",
    "    end_idx = min((fig_num + 1) * subplots_per_fig, np.sum(good_decorr_runs))\n",
    "    \n",
    "    n_rows = math.ceil(math.sqrt(end_idx - start_idx))\n",
    "    n_cols = math.ceil((end_idx - start_idx) / n_rows)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows), dpi=300)\n",
    "    axes = axes.flatten() if isinstance(axes, np.ndarray) else [axes]\n",
    "    \n",
    "    for i, session_n in enumerate(range(start_idx, end_idx)):\n",
    "        for j, mask in enumerate(correlation_matrices):\n",
    "            mean_values_array = all_masks_matrix[j, good_decorr_runs][session_n]\n",
    "            axes[i].plot(np.arange(0, len(mean_values_array)*n_iter, n_iter), mean_values_array, 'o-', label=correlation_names[j])\n",
    "        \n",
    "        axes[i].set_title(f'Run {filtered_indices[session_n]+1}')\n",
    "        axes[i].set_ylim(0, 1)\n",
    "        axes[i].set_xlabel('Epoch')\n",
    "        axes[i].set_ylabel('Mean Correlation')\n",
    "        axes[i].legend()\n",
    "    \n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Decorrelation Analysis (Runs {start_idx+1}-{end_idx} out of {np.sum(good_decorr_runs)} filtered runs)', fontsize=16)\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Decorrelation analysis plots completed.\")\n",
    "print(f\"Total figures generated: {num_figures}\")\n",
    "print(\"These plots show how correlations in different regions change over training epochs for filtered runs.\")\n",
    "print(\"Each line represents a different region of interest in the correlation matrix.\")\n",
    "print(\"Decreasing trends indicate successful decorrelation of representations over time.\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Plot heatmaps for all masks (filtered runs only)\n",
    "for m in range(len(correlation_matrices)):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sb.heatmap(all_masks_matrix[m, good_decorr_runs])\n",
    "    plt.title(f'Mask {m+1} Heatmap: {correlation_names[m]}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Run')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Mask heatmaps plotted for filtered runs.\")\n",
    "print(\"These heatmaps show the evolution of correlations in different regions across runs and epochs.\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Analyze crossing points (filtered runs only)\n",
    "cross_thresh = 0.4\n",
    "run_plot = np.array([True] * np.sum(good_decorr_runs))\n",
    "where_cross = np.zeros((np.sum(good_decorr_runs), len(correlation_matrices)))\n",
    "\n",
    "for m in range(len(correlation_matrices)):\n",
    "    for run in range(np.sum(good_decorr_runs)):\n",
    "        cross_points = np.where(all_masks_matrix[m, good_decorr_runs][run] < cross_thresh)[0]\n",
    "        if len(cross_points) > 0:\n",
    "            where_cross[run, m] = cross_points[0]\n",
    "        else:\n",
    "            where_cross[run, m] = all_masks_matrix.shape[2]\n",
    "            run_plot[run] = False\n",
    "\n",
    "where_cross = where_cross[run_plot]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_plot = np.repeat(np.array([0,1,2])[:,np.newaxis], where_cross.shape[0], axis=1) + np.random.normal(0,0.05,(3,where_cross.shape[0]))\n",
    "plt.barh([0,1,2], [np.mean(where_cross[:,0]), np.mean(where_cross[:,1]), np.mean(where_cross[:,2])], color='lightblue')\n",
    "plt.plot(np.vstack((where_cross[:,0], where_cross[:,1], where_cross[:,2])), x_plot, color='gray')\n",
    "plt.plot(np.vstack((where_cross[:,0], where_cross[:,1], where_cross[:,2])), x_plot, 'o', color='black')\n",
    "plt.yticks([0,1,2], correlation_names)\n",
    "plt.title('Crossing Points Analysis (Filtered Runs)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()\n",
    "\n",
    "print(\"Crossing points analysis completed and plotted for filtered runs.\")\n",
    "print(\"This plot shows when different regions' correlations cross the threshold of 0.4.\")\n",
    "print(f\"Mean crossing epoch for {correlation_names[0]}: {np.mean(where_cross[:,0]):.2f}\")\n",
    "print(f\"Mean crossing epoch for {correlation_names[1]}: {np.mean(where_cross[:,1]):.2f}\")\n",
    "print(f\"Mean crossing epoch for {correlation_names[2]}: {np.mean(where_cross[:,2]):.2f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(\"Analysis completed.\")\n",
    "print(f\"Summary:\")\n",
    "print(f\"- Total simulations: {corr_curve.shape[0]}\")\n",
    "print(f\"- Runs with loss < {loss_thresh}: {good_runs}\")\n",
    "print(f\"- Runs passing both loss and decorrelation thresholds: {np.sum(good_decorr_runs)}\")\n",
    "print(f\"- Final mean loss (all runs): {np.mean(loss_all[:,-1]):.4f}\")\n",
    "print(f\"- Final mean loss (filtered runs): {np.mean(loss_all[filtered_indices, -1]):.4f}\")\n",
    "print(f\"- Final mean accuracy (all runs): {np.mean(accuracy_curve_all_test[:,-1]):.4f}\")\n",
    "print(f\"- Final mean accuracy (filtered runs): {np.mean(accuracy_curve_all_test[filtered_indices, -1]):.4f}\")\n",
    "print(f\"- Mean last-step correlation (filtered runs): {np.mean(corr_all):.4f}\")\n",
    "print(\"The analysis suggests that the model is learning to decorrelate representations over time, with different regions showing distinct patterns of decorrelation.\")\n",
    "print(f\"Decorrelation is particularly strong in runs that pass both the loss threshold of {loss_thresh} and the decorrelation threshold of {decorr_thresh}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13701e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Analysis 1: Correlation heatmaps at different epochs\n",
    "\n",
    "# Calculate mean across filtered runs\n",
    "mean_corr_curve = np.mean(corr_curve[filtered_indices], axis=0)\n",
    "\n",
    "# Choose 5 epochs to visualize, including the last one\n",
    "num_epochs = mean_corr_curve.shape[0]\n",
    "epochs_to_plot = [0, num_epochs // 4, num_epochs // 2, 3 * num_epochs // 4, num_epochs - 1]\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 4), dpi=600)\n",
    "\n",
    "for i, epoch in enumerate(epochs_to_plot):\n",
    "    sns.heatmap(mean_corr_curve[epoch, 0:tr_len, tr_len:2*tr_len], \n",
    "                cmap='icefire', vmin=-1, vmax=1, ax=axs[i], \n",
    "                cbar=False, xticklabels=False, yticklabels=False, linewidths=0)\n",
    "    axs[i].set_aspect('equal')  # make each subplot square\n",
    "    axs[i].set_title(f'Epoch {epoch + 1}')\n",
    "    \n",
    "    # Loop for dotted lines\n",
    "    for lines in [6, 10, 13, 15, 18, 20]:  \n",
    "        axs[i].axvline(lines, linestyle=(0, (2, 5)), color='white', linewidth=1.5)  # dotted vertical lines\n",
    "        axs[i].axhline(lines, linestyle=(0, (2, 5)), color='white', linewidth=1.5)  # dotted horizontal lines\n",
    "    \n",
    "    # Draw square bounding box\n",
    "    for (low, high) in [(6, 10), (13, 15), (18, 20)]:\n",
    "        axs[i].plot([low, high, high, low, low], [low, low, high, high, low], color='white', linewidth=3)  \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Get today's date for the filename\n",
    "today = datetime.now().strftime('%Y_%m_%d')\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(f'Polynomial_RNN_corr_plot_{today}.pdf', format='pdf', dpi=600)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation heatmaps at different epochs plotted and saved.\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Analysis 2: Normalized crossing points analysis\n",
    "\n",
    "# Analyze crossing points (filtered runs only)\n",
    "cross_thresh = decorr_thresh\n",
    "run_plot = np.array([True] * np.sum(good_decorr_runs))\n",
    "where_cross = np.zeros((np.sum(good_decorr_runs), len(correlation_matrices)))\n",
    "\n",
    "for m in range(len(correlation_matrices)):\n",
    "    for run in range(np.sum(good_decorr_runs)):\n",
    "        cross_points = np.where(all_masks_matrix[m, good_decorr_runs][run] < cross_thresh)[0]\n",
    "        if len(cross_points) > 0:\n",
    "            where_cross[run, m] = cross_points[0]\n",
    "        else:\n",
    "            where_cross[run, m] = all_masks_matrix.shape[2]\n",
    "            run_plot[run] = False\n",
    "\n",
    "where_cross = where_cross[run_plot]\n",
    "\n",
    "# Normalize cross times\n",
    "max_cross_times = np.max(where_cross, axis=1) + 2\n",
    "normalized_cross = where_cross / max_cross_times[:, np.newaxis]\n",
    "\n",
    "# Reorder the data\n",
    "new_order = [0, 2, 1]  # Off-diagonal, Pre-R1, Pre-R2\n",
    "reordered_normalized_cross = normalized_cross[:, new_order]\n",
    "reordered_correlation_names = [correlation_names[i] for i in new_order]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_plot = np.repeat(np.array([0,1,2])[:,np.newaxis], reordered_normalized_cross.shape[0], axis=1) + np.random.normal(0,0.05,(3,reordered_normalized_cross.shape[0]))\n",
    "plt.barh([0,1,2], [np.mean(reordered_normalized_cross[:,0]), np.mean(reordered_normalized_cross[:,1]), np.mean(reordered_normalized_cross[:,2])], color='lightblue')\n",
    "plt.plot(np.vstack((reordered_normalized_cross[:,0], reordered_normalized_cross[:,1], reordered_normalized_cross[:,2])), x_plot, color='gray')\n",
    "plt.plot(np.vstack((reordered_normalized_cross[:,0], reordered_normalized_cross[:,1], reordered_normalized_cross[:,2])), x_plot, 'o', color='black')\n",
    "plt.yticks([0,1,2], reordered_correlation_names)\n",
    "plt.title('Normalized Crossing Points Analysis (Filtered Runs)')\n",
    "plt.xlabel('Normalized Epoch')\n",
    "plt.xlim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "print(\"Normalized crossing points analysis completed and plotted for filtered runs.\")\n",
    "print(f\"This plot shows when different regions' correlations cross the threshold of {decorr_thresh}, normalized by the longest cross time +2 for each run.\")\n",
    "print(f\"Mean normalized crossing time for {reordered_correlation_names[0]}: {np.mean(reordered_normalized_cross[:,0]):.2f}\")\n",
    "print(f\"Mean normalized crossing time for {reordered_correlation_names[1]}: {np.mean(reordered_normalized_cross[:,1]):.2f}\")\n",
    "print(f\"Mean normalized crossing time for {reordered_correlation_names[2]}: {np.mean(reordered_normalized_cross[:,2]):.2f}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ff46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Calculate mean normalized crossing times\n",
    "mean_crossing_times = np.mean(reordered_normalized_cross, axis=0)\n",
    "\n",
    "print(\"Mean normalized crossing times:\")\n",
    "for name, mean_time in zip(reordered_correlation_names, mean_crossing_times):\n",
    "    print(f\"{name}: {mean_time:.4f}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Perform t-tests between pairs of crossing times\n",
    "pairs = [(0, 1), (0, 2), (1, 2)]\n",
    "pair_names = [\n",
    "    ('Off-diagonal', 'Pre-R1'),\n",
    "    ('Off-diagonal', 'Pre-R2'),\n",
    "    ('Pre-R1', 'Pre-R2')\n",
    "]\n",
    "\n",
    "print(\"T-test results for normalized crossing times:\")\n",
    "for (i, j), (name1, name2) in zip(pairs, pair_names):\n",
    "    t_stat, p_value = stats.ttest_rel(reordered_normalized_cross[:, i], reordered_normalized_cross[:, j])\n",
    "    n = reordered_normalized_cross.shape[0]\n",
    "    print(f\"\\nComparing {name1} vs {name2}:\")\n",
    "    print(f\"t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"p-value: {p_value:.4e}\")\n",
    "    print(f\"N: {n}\")\n",
    "    \n",
    "    # Interpret the results\n",
    "    if p_value < 0.05:\n",
    "        if mean_crossing_times[i] < mean_crossing_times[j]:\n",
    "            print(f\"{name1} decorrelates significantly earlier than {name2}\")\n",
    "        else:\n",
    "            print(f\"{name2} decorrelates significantly earlier than {name1}\")\n",
    "    else:\n",
    "        print(f\"No significant difference in decorrelation timing between {name1} and {name2}\")\n",
    "\n",
    "print(\"\\nNote: A small p-value (< 0.05) indicates a significant difference between the crossing times.\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Overall interpretation\n",
    "print(\"Summary of decorrelation order:\")\n",
    "sorted_indices = np.argsort(mean_crossing_times)\n",
    "for i, idx in enumerate(sorted_indices):\n",
    "    print(f\"{i+1}. {reordered_correlation_names[idx]} (mean crossing time: {mean_crossing_times[idx]:.4f})\")\n",
    "\n",
    "print(\"\\nStatistically significant differences:\")\n",
    "for (i, j), (name1, name2) in zip(pairs, pair_names):\n",
    "    if stats.ttest_rel(reordered_normalized_cross[:, i], reordered_normalized_cross[:, j])[1] < 0.05:\n",
    "        if mean_crossing_times[i] < mean_crossing_times[j]:\n",
    "            print(f\"- {name1} decorrelates significantly earlier than {name2}\")\n",
    "        else:\n",
    "            print(f\"- {name2} decorrelates significantly earlier than {name1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47645dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "reordered_normalized_cross.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c315da",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45be320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
